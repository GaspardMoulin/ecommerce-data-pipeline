# E-commerce Data Extraction Pipeline

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![Status](https://img.shields.io/badge/Status-Production%20Ready-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

Advanced multi-source e-commerce scraping pipeline with anti-bot handling, data cleaning, and automated monitoring. Extracts 1,000+ products from API endpoints and web sources, delivering clean, analysis-ready datasets.

## ğŸ¯ Project Overview

This project demonstrates professional-grade data extraction capabilities by:
- âœ… Scraping multiple data sources (API + Web)
- âœ… Implementing anti-detection mechanisms
- âœ… Performing comprehensive data cleaning and normalization
- âœ… Exporting to multiple formats (CSV, Excel, JSON)
- âœ… Providing detailed logging and error handling
- âœ… Offering flexible command-line interface

## ğŸ“Š Key Features

### Multi-Source Data Collection
- **API Scraping**: DummyJSON API with pagination support
- **Web Scraping**: Books to Scrape website with dynamic content handling
- **Total Products**: 1,000+ products extracted and processed

### Advanced Scraping Techniques
- User-Agent rotation
- Random delays to mimic human behavior
- Retry logic with exponential backoff
- Comprehensive error handling
- Respectful rate limiting

### Data Processing
- Automatic data cleaning and validation
- Missing value handling
- Data type normalization
- Calculated fields generation
- Multi-format export (CSV, Excel, JSON)

### Professional Architecture
- Modular design with separation of concerns
- Comprehensive logging system
- Configuration management
- Command-line interface
- Reusable components

## ğŸš€ Quick Start

### Prerequisites
```bash
Python 3.9+
pip
```

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/ecommerce-data-pipeline.git
cd ecommerce-data-pipeline
```

2. Create virtual environment:
```bash
python -m venv venv
```

3. Activate virtual environment:
```bash
# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate
```

4. Install dependencies:
```bash
pip install -r requirements.txt
```

### Basic Usage

Run the full pipeline with default settings:
```bash
python main.py
```

This will:
- Extract 100 products from API
- Scrape 100 products from web
- Clean and merge the data
- Export to CSV, Excel, and JSON

## ğŸ“– Usage Examples

### Command-Line Interface

Extract specific quantities:
```bash
python main.py --api-products 500 --web-products 200
```

Limit web scraping pages:
```bash
python main.py --web-pages 5
```

Extract from API only:
```bash
python main.py --api-only --api-products 1000
```

Extract from web only:
```bash
python main.py --web-only --web-products 500
```

### Predefined Configurations

Quick test (minimal data):
```bash
python run_configs.py quick
```

Medium extraction:
```bash
python run_configs.py medium
```

Full extraction (all available data):
```bash
python run_configs.py full
```

## ğŸ“ Project Structure
```
ecommerce-scraper/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ api_scraper.py       # API extraction logic
â”‚   â””â”€â”€ web_scraper.py       # Web scraping logic
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_cleaner.py      # Data processing utilities
â”‚   â”œâ”€â”€ logger.py            # Logging configuration
â”‚   â””â”€â”€ web_helpers.py       # Web scraping helpers
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py          # Configuration settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Raw scraped data
â”‚   â”œâ”€â”€ processed/           # Cleaned datasets
â”‚   â””â”€â”€ images/              # Downloaded images
â”œâ”€â”€ logs/                    # Application logs
â”œâ”€â”€ main.py                  # Main pipeline orchestrator
â”œâ”€â”€ run_configs.py           # Predefined run configurations
â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸ”§ Configuration

Edit `config/settings.py` to customize:
- Request delays and timeouts
- User agent rotation
- File paths
- Database settings
- Scraping limits

## ğŸ“Š Output Files

After running the pipeline, check `data/processed/`:
- `ecommerce_products_TIMESTAMP.csv` - Main dataset in CSV
- `ecommerce_products_TIMESTAMP.xlsx` - Excel workbook
- `ecommerce_products_TIMESTAMP.json` - JSON format
- `statistics_TIMESTAMP.json` - Dataset statistics
- `DATA_ANALYSIS_REPORT.md` - Comprehensive analysis report

## ğŸ› ï¸ Tech Stack

- **Python 3.9+**
- **Requests** - HTTP library
- **BeautifulSoup4** - HTML parsing
- **Scrapy** - Web crawling framework
- **Selenium** - Browser automation
- **Playwright** - Modern browser automation
- **Pandas** - Data manipulation
- **Openpyxl** - Excel file handling

## ğŸ“ˆ Performance

- **Extraction Speed**: ~50-100 products/minute
- **Success Rate**: >95% with retry logic
- **Data Quality**: 99%+ completeness rate
- **Scalability**: Tested with 1,000+ products

## ğŸ” Data Quality Assurance

- Automatic duplicate removal
- Missing value handling
- Data type validation
- Outlier detection
- Consistency checks

## ğŸ“ Logging

Comprehensive logging system with:
- Timestamped entries
- Multiple log levels (INFO, WARNING, ERROR)
- Separate log files per module
- Console and file output

Check logs in `logs/` directory.

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ‘¤ Author

**Gaspard MOULIN**
- LinkedIn: https://www.linkedin.com/in/gaspard-moulin-213bab225/
- GitHub: https://github.com/GaspardMoulin

## ğŸ™ Acknowledgments

- DummyJSON for providing free API
- Books to Scrape for test website
- Open-source community for excellent tools

## ğŸ“ Contact

For questions or collaboration opportunities, reach out via:
- Email: gaspard.moulin@sportifgourmand.com
- LinkedIn: https://www.linkedin.com/in/gaspard-moulin-213bab225/

---

â­ If you find this project useful, please consider giving it a star!